# LangChain4j with Docker: Connect to Local LLMs

This repository demonstrates how to connect a LangChain4j application to a local Large Language Model (LLM) running inside Docker Desktop. Build private, offline, and zero-cost AI applications.

ğŸ“– **Complete Guide**: For detailed explanations and a full code walkthrough, read our comprehensive tutorial.<br>
ğŸ‘‰ [**LangChain4j ChatModel Listeners: A Complete Guide**](https://bootcamptoprod.com/langchain4j-with-docker/)

ğŸ¥ **Video Tutorial**: Prefer hands-on learning? Watch our step-by-step implementation guide.<br>
ğŸ‘‰ YouTube Tutorial - Coming Soon!!

---

## âœ¨ What This Project Demonstrates

This application is a practical guide to connecting LangChain4j application with AI models that are running locally inside Docker, covering:

- **Docker Model Runner Integration**: Utilize Docker Desktop's built-in GenAI features to serve models.
- **Private AI Execution**: Run models like Gemma or Llama 3 completely offline using Docker.
- **Zero-Cost Development**: Experiment with LLMs without worrying about API bills or rate limits.
- **LangChain4j Configuration**: Learn how to point the `langchain4j-open-ai-spring-boot-starter` to a local, OpenAI-compatible endpoint.

---

## ğŸ› ï¸ Prerequisites & Setup

To run this application, you will need the following:

1.  **Java 21+ & Maven**: Ensure you have Java 21 and Maven installed.
2.  **Docker Desktop**: Install the latest version of Docker Desktop.
3.  **Local Model Configuration**: You must have a local LLM downloaded and exposed via TCP in Docker Desktop.

**For detailed instructions on how to download a model and configure Docker, please follow the setup guide in our comprehensive article:**<br>
ğŸ‘‰ [**Click here for Detailed Instructions**](https://bootcamptoprod.com/langchain4j-with-docker/)

---